#SAREBBE UTILE SCRIVERE DEL CODICE PER VERIFICARE GLI NA

 

#Library----

library(terra) # methods for spatial data analysis with vector (points, lines, polygons) and raster (grid) data. Methods for vector data include geometric operations such as intersect and buffer. Raster methods include local, focal, global, zonal and geometric operations. The predict and interpolate methods facilitate the use of regression type (inrerpolation, machine learning) medels for spatial prediction, including with satellite remote sensing data. Manual and tutorials on <https://rspatial.org/>

library(pROC) # R package to display and analyze ROC (receiving operating characteristics) curves

library(caret) # Classification And REgression Training is a set of functions that attempt to streamiline the process for creating predictive models. The package contains tools for: data splitting, pre-processing, feature selection, model tuning using resampling, variable importance estimation

library(sf) # support for simple features, a standardized way to encode spatial vector data. Binds to GDAL for reading and writing data, to GEOS for geometrical operations, and to PROJ for projection conversions and datum transformations.

library(purrr) # enhances R functional programming toolkit by providing set of tools for working with functions and vectors. 

library(tools) # tools for package development, administration and documentation

library(tidyverse) # set of packages that work in harmony because they share common data representations and 'API' design. This package is designed to make it easy to install and load multiple tydiverse packages in a single step.

library(rsample) # classes and functions to create and summarize different types of resampling objects (e.g. bootstrap, cross-validation)

library(furrr) # implementations of the family of map() functions from 'purrr' that can be resolved using any future supported backend, e.g. parallel on the local machine or distributed on a computer cluster.

library(pbapply) # package that add progress bar to vectorized R functions ('*apply'). The implementation can easily be added to functions where showing the progress is useful (e.g. bootstrap). The type and style of the progress bar (with percentages or remaining time) can be set through options. Support several parallel process backends including future.

library(ranger) # a fast implementation of Random Forests, particularly suited for high dimensional data. Ensembles of classification, regression, survival and probability prediction trees are supported.

library(tidymodels) # collection of packages for modelling and machine learning using tydiverse principles.

library(e1071) # functions for latent class analysis, short time Fourier transform, fuzzy clustering, support vector machines, shortest path computation, bagged clustering, naive Bayes classifier, generalized k-nearest neighbour...

 

#Functions----

source("R/clean_name.R") #scorciatoia per :

# clean_name <- function(file) {                          # NON CAPITA

#   var_name <- tools::file_path_sans_ext(basename(file)) # :: is an operator which helps to access a specific function from a specific package.

#   var_name <- gsub("[^[:alnum:]_]", "", var_name)

#   if (grepl("^[0-9]", var_name)) {

#     var_name <- paste0("x", var_name)

#   }

#   var_name

# }

 

#Load data----

ortho <- list.files(pattern = "Ortho", full.names = TRUE, recursive = TRUE)%>% # this is a pipe operator, it takes the output of one function and passes it into another function as an argument.

  set_names(nm = map(., clean_name)) %>%  # nm = a character vector of names to assign to the object.

  map(~rast(.))

 

buffers <- list.files(pattern = "Buffer.*shp",                       # list.files = it produces a character vector of the names of files or directories in the named directory

                      full.names = TRUE, recursive = TRUE)%>%

  set_names(nm = map(., clean_name)) %>%    # il . fa forse riferimento alla pipe?

  map(~st_read(.))                       # map(): apply a function to each element of a vector. st_read: read simple features from file or database.

# remove EL2, EY3, RA2 (2, 5, 10)

buffers[[10]] <- NULL

buffers[[5]] <- NULL

buffers[[2]] <- NULL

 
# con la funzione map (purrr) applichiamo la funzione ext() a ciascun elemento del vettore buffers. La funzione ext() restituirà l'estensione spaziale di ciascun oggetto della lista buffers.
# map2 (purrr) applica la funzione crop() ai dati dell'oggetto ortho e ai risultati ottenuti dalla prima parte.
# La funzione crop(.x, .y) prende due argomenti: il primo argomento (ortho) è il dato che verrà ritagliato, il secondo argomento ('...') è l'estensione specificata per il ritaglio.
# In pratica la funzione crop() sta ritagliando il dato ortho in base alle estensioni spaziali specificate nell'output della prima parte.
# In sintesi ritagliamo i dati spaziali nell'oggetto ortho con l'estensione dell'oggetto buffers.

ortho<-map2(ortho, map(buffers, ~ ext(.x)), ~ crop(.x, .y))    # ext(): get a SpatExtent of a SpatRaster, SpatVector or other spatial objects.


# la seguente linea di codice esegue un'operazione di mascheramento su dati spaziali rappresentati in ortho.
# Per ogni coppia di elementi da ortho e buffers la funzione mask() viene applicata per mascherare il dato spaziale in ortho utilizzando l'oggeto spaziale in buffers.
# Il risulatato viene assegnato nuovamente ad ortho sovrascrivendo il dato originale con i dati mascherati.
# Il mascheramento può essere utilizzato per escludere o isolare una determinata area di interesse nei dati spaziali.

ortho<-map2(ortho, buffers, ~ mask(.x, .y))                    # map2: map over multiple inputs simultaneously. Mask x = ortho utilizzando y = buffers

# mask(): create a new Raster object that has the same values as 'x', except for the cells that are 'NA' in a 'mask'. These cells become 'NA'. The mask can be either another Raster object of the same extent and resolution, or a Spatial* object in which 
case all cells are not covered by the spatial object are set to 'updatevalue'. You can use 'inverse=TRUE' to set the cells that are not 'NA' (or other 'maskvalue') in the mask, or not covered by the Spatial*object, to 'NA' (or other update value).

#Vogliamo sapere se lo sfondo del nostro transetto sia composto da NA per

#escluderli successivamente

 

outside_na <- map2(ortho, buffers, function(ortho, buffer) {    # = apply the function that is being created to the vectors ortho and buffers

  # Converti il buffer in SpatVector, is used as a spatial filter to select geometries to read.

  buffer <- terra::vect(buffer)

 

  # Crea un raster di stesse dimensioni dell'immagine originale e riempilo con 1

  base_raster <- ortho * 0 + 1

 

  # Applica la maschera all'immagine creata

  mask_inverse <- terra::mask(base_raster, buffer, inverse = TRUE)

 

  # Estrae i valori al di fuori del buffer

  outside_values <- terra::values(mask_inverse)

 

  # Verifica se questi valori sono tutti NA

  all(is.na(outside_values))

})

 

# Stampa i risultati

print(outside_na)

 

 

#Fiori ed Erba

 

 

fiori<- list.files(pattern = "fiori.*shp", full.names = TRUE, recursive = TRUE)%>%

  set_names(nm = map(., clean_name)) %>%

  map(~st_read(.))%>%

  map(~.[!st_is_empty(.), ])           # st_is_empty return for each geometry whether it is empty. ! is the logical operator for negation: !(5>1) return FALSE while "!(1>5)" returns TRUE

 

erba<-list.files(pattern = "erba.*shp", full.names = TRUE, recursive = TRUE)%>%

  set_names(nm = map(., clean_name)) %>%   # list.files produces a character vector of the names of files or directories in the named directory

  map(~st_read(.))%>%                      # read simple features from file or database, or retrieve layer names and their geometry type(s)

  map(~.[!st_is_empty(.), ]) 

 

#Ordiniamo i nomi in modo che coincidano

names(ortho) <- sort(names(ortho))        # order a vector or factor into ascending or descending order.

names(fiori) <- sort(names(fiori))

names(erba) <- sort(names(erba))

 

#Values extraction----
# estraiamo i valori di ortho relativi ai pixels interni ai poligoni classificati come fiori
df_fiori <- map2(ortho, fiori, ~terra::extract(.x, .y))     # extract a character column into multiple columns using regular regression groups
# stesso per pixels appartenenti ai poligoni di erba
df_erba <- map2(ortho, erba, ~terra::extract(.x, .y))

 

 

#Nomi delle colonne di ogni dataframe

df_fiori <- map(df_fiori, ~{

  colnames(.) <- c("ID_poly", paste0("banda_", 1:(ncol(.)-1)))      # sceglie nomi per ciascuna colonna del dataframe

  .

})

 

df_erba <- map(df_erba, ~{

  colnames(.) <- c("ID_poly", paste0("banda_", 1:(ncol(.)-1)))

  .

})

 

 

#label: aggiungiamo una colonna contenente la label per ciascun dataframe

df_fiori <- map(df_fiori, ~{

  .$label <- "fiore"

  .

})

 

df_erba <- map(df_erba, ~{

  .$label <- "erba"

  .

})

 

#Bind dataframes----

 

# Unisci tutti i dataframe in una lista
# .id ="origin" intende aggiungere una nuova colonna dove per ogni pixel sia identificata l'immagine di appartenza
df_fiori_unified <- bind_rows(df_fiori, .id = "origin")

df_erba_unified <- bind_rows(df_erba,.id = "origin")

 

 

# Numero di righe da campionare da ciascun gruppo

sample_size <- 90000

 

# Crea un campione bilanciato

df_erba_unified <- df_erba_unified %>%

  group_by(origin) %>%

  sample_n(size = sample_size, replace = TRUE) %>%    # sample_n() (dplyr) è usata per campionare un numero specifico di righe da un dataframe. 
# replace = TRUE signica che la funzione campionerà casualmente un numero di righe pari a sample size utilizzando la tecnica del campionamento con sostituzione
# il che significa che le stesse righe potrebbero apparire più di una volta nel campione risultante. È utile quando si desidera effettuare campionamenti casuali ripetuti 
# o quando si vuole consentire la possibilità di estrarre più volte le stesse righe.

  ungroup()    # consente di tornare al data frame originale senza gruppi, rendendo più facile eseguire ulteriori operazioni o analisi sui dati in modo non raggruppato

 

 

df_tot <- rbind(df_fiori_unified, df_erba_unified) %>%

  rowid_to_column(.) %>%            # funzione del pacchetto dplyr che serve a convertire le righe ID in una colonna separata all'interno del dataframe.
                                      è utile quando si desidera trasformare l'ID della riga in una variabile del dataframe, rendendola parte dei dati.

  rename(ID_pixel = rowid)

 

# Crea un dataframe 'lungo' #SOLO SE VEDIAMO NA

# df_long <- df_tot %>%

#   tidyr::pivot_longer(cols =c("banda_1", "banda_2", "banda_3"),

#                       names_to = "banda",

#                       values_to = "valore")

# df_long %>%

#   filter(is.na(valore)) %>%

#   view()

 

 

#Splitting into train and test----

#Seleziono le bande

 

set.seed(123)

 

# Crea un nuovo dataframe con solo le colonne che vuoi usare

df_selected <- df_tot %>%

  select(banda_1,banda_2 , banda_3, label)     # funzione del pacchetto dplyr utilizzata per selezionare e restituire un sottoinsieme specifico delle colonne da un dataframe.
                                                 select(data, col1, col2, ...)

 

# Dividi il dataframe selezionato in set di addestramento e di test

data_split <- initial_split(df_selected, prop = 0.7, strata = "label")   # prendiamo il 70% dei dati in df_selected per utilizzarli come set di training

training_data <- training(data_split)    # estraiamo i dati di addestramento da quel 70% del totale

test_data <- testing(data_split)     # estraiamo i dati di test dallo stesso 70% del totale

 

# Convertiamo in un fattore

training_data$label <- as.factor(training_data$label)

test_data$label <- as.factor(test_data$label)

 

#RF Model----

# la funzione expand.grid è utilizzata per creare un dataframe che rappresenta tutte le combinazioni possibili di un insieme di vettori o fattori forniti come argomenti.
# Questo è particolarmente utile quando si desidera esplorare tutte le combinazioni di variabili in un esperimento o in una griglia di parametri per la ricerca di modelli.
# le seguenti linee di codice creano un "tune grid" utilizzato per l'ottimizzazione dei parametri in un modello di Random Forest
# o un modello basato su alberi. Questo tunegrid rappresenta una griglia di combinazioni dei parametri da valutare al fine di determinare quale combinazione
# produce le migliori prestazioni per il modello. 

tuneGrid <- expand.grid(.mtry = c(1, 2, 3),    # .mtry rappresenta il numero di variabili casuali da selezionare per la suddivisione dei nodi durante la costruzione di alberi casuali. In questo caso consideriamo tre valori possibili: 1, 2 e 3.

                        .splitrule = "hellinger",     # spitrule è un parametro che specifica la regola da utilizzare per la suddivisione dei nodi degli alberi casuali. 

                        .min.node.size = c(1, 3, 5))     # min.node.size rappresenta la dimensione minima di un nodo nell'albero. Questo parametro controlla quante osservazioni minime devono essere presenti in un nodo prima che possa essere suddiviso ulteriormente. Qui si stanno considerando 3 possibili valori: 1, 3 e 5

 

# tuneGrid <- expand.grid(.mtry = c(1, 2, 3),

#                         .splitrule ="gini",

#                         .min.node.size = c(1, 3, 5))

 

# Define cross-validation strategy
# le seguenti linee di codice configurano i parametri di controllo per l'addestramento di modelli di machine learning
# utilizzando il pacchetto caret. In particolare, questi parametri vengono utilizzati durante il processo di cross-validation
# per la valutazione delle prestazioni del modello.
# La cross-validation verrà eseguita con 10 folds, le previsioni verranno salvate alla fine di ogni fold e verranno visualizzati messaggi dettagliati durante il processo di addestramento.
# Questi parametri sono utilizzati per gestire il processo di addestramento e valutazione del modello in modo più controllato e informato.

train_control <- trainControl(method = "cv", number = 10,   # method specifica il metodo di cross-validation da utilizzare. CV sta per cross-validation. 
                                                              la cv è un approccio comune per valutare le prestazioni di un modello, suddividendo il dataset in diverse parti
                                                              e addestrando il modello su una parte e valutandolo sull'altra, ripetendo questo processo più volte.

                              savePredictions = "final",    # savePredictions è un parametro che specifica quando le previsioni dovrebbero essere salvate. Final indica che le previsioni saranno salvate alla fine del processo di cross-validation.

                              verboseIter = TRUE)    # parametro booleano che controlla se dovrebbero essere visualizzati messaggi di output dettagliati durante il processo di addestramento del modello.

 

## Train the model using ranger and the tuning grid

# Calcola i pesi delle classi
# il codice seguente estrae le classi uniche da un dataset di addestramento e crea un vettore di pesi per ciascuna classe, assegnando pesi specifici
# alle classi in modo da influenzare il comportamento del modello di classificazione durante l'addestramento.

classi <- unique(training_data$label)  # la funzione unique() viene usata per estrarre i valori unici dalla colonna 'label' del dataframe 'training data'.
                                       # I valori unici vengono quindi assegnati all'oggetto 'classi', creando un vettore con le etichette uniche delle classi.

class_weights <- c("erba" = 0.1, "fiore" = 0.9)  # qui si assegnano i pesi specifici a ciascuna classe del problema di classificazione. Attribuendo un peso maggiore ad una classe,
                                                 # il modello darà più importanza a quella classe nella fase di addestramento e ottimizzazione dei parametri.

 

# Addestra il modello utilizzando i pesi delle classi
# il codice addestra un modello Random Forest utilizzando il pacchetto caret e il metodo ranger.
# train() è una funzione del pacchetto caret utilizzata per addestrare modelli di machine learning.

RF <- train(label ~ ., data = training_data,     # label ~ indica che label è la variabile di risposta da prevedere e "." significa che tutte le altre variabili nel dataframe 'training data' sono utilizzate come variabili predittorie.

            method = "ranger",                   # method indica il metodo di addestramento da utilizzare, in questo caso il random forest.

            num.trees = 500,                     # num.trees specifica il numero di alberi da creare nel modello RF.

            trControl = train_control,           # trControl specifica i parametri di controllo dell'addestramento come i dettagli della cross-validation, l'output dettagliato ecc.

            tuneGrid = tuneGrid,                 # tuneGrid è l'oggetto precedentemente creato ed utilizzato per l'ottimizzazione dei parametri del modello.

            class.weights = class_weights)

 

# t0<-Sys.time()

# RF <- train(label ~ ., data = training_data,

#             method = "ranger",

#             num.trees = 500,

#             trControl = train_control,

#             tuneGrid = tuneGrid)

# t1<-Sys.time()

# t1-t0

 

print(RF)

 

#se volessi addestrare un modello SVM, potrei usare method = "svmLinear"

 

#RF model test----

predizioni <- predict(RF, test_data)

matrice_confusione <- confusionMatrix(predizioni, test_data$label, positive = "fiore")

print(matrice_confusione)

accuracy <- matrice_confusione$overall['Accuracy']

recall <- matrice_confusione$byClass["Sensitivity"]

f1 <- matrice_confusione$byClass["F1"]

auc_roc<- roc(test_data$label, as.numeric(predizioni))

auc <- auc(auc_roc)

precision <- matrice_confusione$byClass["Pos Pred Value"]

 

 

print(matrice_confusione)

print(paste("Accuracy: ", accuracy))

print(paste("Recall: ", recall))

print(paste("F1-Score: ", f1))

print(paste("AUC-ROC: ", auc))

print(paste("Precision: ", precision))

 

 

 

#Funzione a tutti gli elementi in ortho

ortho <- map(ortho, function(x) {

  names(x) <- colnames(df_selected)[1:3]

  return(x)

})

 

 

#Application of model and save----

t0<-Sys.time()

classificazione_RF_list <- pbapply::pblapply(names(ortho), function(name) {

  terra::predict(ortho[[name]], RF, na.rm=TRUE) #gli NA sono dello sfondo, quindi possiamo escluderli

})

t1<-Sys.time()

t1-t0

 

#Time difference of 30.24887 mins

 

#QUI

 

#classificazione_RF_list <- future_map(ortho[[1]], ~predict(., RF),.progress = TRUE)

 

 

# Salva la classificazione come un file GeoTIFF

#output_filename <- "classificazione_RF.tif"

#writeRaster(classificazione_RF, output_filename, overwrite = TRUE)

 

 

names(classificazione_RF_list) <- names(ortho)  # Assicuriamoci che i nomi corrispondano

 

 

walk(names(classificazione_RF_list), function(name) {

  terra::writeRaster(classificazione_RF_list[[name]],

                     filename = paste0(name, ".tif"),

                     overwrite=TRUE)

})

 

 

 

 

 

#ATTENZIONE proviamo a contare le aree----

 

# Carico la libreria

library(terra)

 

# Assumendo che i raster che devi analizzare siano già presenti nel tuo environment,

# non c'è bisogno di listare e caricare i file come hai fatto nel tuo script originale.

 

# Creo una struttura dati vuota per memorizzare i risultati

res_fiori <- data.frame(

  fiori = numeric(0),

  erba = numeric(0),

  area = character(0)

)

 

# Supponendo che i tuoi raster siano in un oggetto chiamato "classificazione_RF_list"

for (i in 1:length(classificazione_RF_list)) {

  raster_current <- classificazione_RF_list[[i]]

 areaname <- names(classificazione_RF_list)[i]

  ncell(raster_current)

 

  # Convertire il raster in poligoni

  p <- as.polygons(raster_current)

 

  # Contare le celle per ogni categoria

  count <- expanse(p)

 

  # Assumendo che "fiori" siano etichettati come 1 e "erba" come 2 nel tuo raster

  count_fiori <- count[1]

  count_erba <- count[2]

 

  # Aggiungere i risultati al dataframe

  res_fiori <- rbind(res_fiori, data.frame(

    fiori = count_fiori,

    erba = count_erba,

    area = areaname

  ))

}

 

# Salvare i risultati

write.table(res_fiori, "res_fiori.csv", row.names = FALSE)
