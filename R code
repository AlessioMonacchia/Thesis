#SAREBBE UTILE SCRIVERE DEL CODICE PER VERIFICARE GLI NA

 

#Library----

library(terra) # methods for spatial data analysis with vector (points, lines, polygons) and raster (grid) data. Methods for vector data include geometric operations such as intersect and buffer. Raster methods include local, focal, global, zonal and geometric operations. The predict and interpolate methods facilitate the use of regression type (inrerpolation, machine learning) medels for spatial prediction, including with satellite remote sensing data. Manual and tutorials on <https://rspatial.org/>

library(pROC) # R package to display and analyze ROC (receiving operating characteristics) curves

library(caret) # Classification And REgression Training is a set of functions that attempt to streamiline the process for creating predictive models. The package contains tools for: data splitting, pre-processing, feature selection, model tuning using resampling, variable importance estimation

library(sf) # support for simple features, a standardized way to encode spatial vector data. Binds to GDAL for reading and writing data, to GEOS for geometrical operations, and to PROJ for projection conversions and datum transformations.

library(purrr) # enhances R functional programming toolkit by providing set of tools for working with functions and vectors. 

library(tools) # tools for package development, administration and documentation

library(tidyverse) # set of packages that work in harmony because they share common data representations and 'API' design. This package is designed to make it easy to install and load multiple tydiverse packages in a single step.

library(rsample) # classes and functions to create and summarize different types of resampling objects (e.g. bootstrap, cross-validation)

library(furrr) # implementations of the family of map() functions from 'purrr' that can be resolved using any future supported backend, e.g. parallel on the local machine or distributed on a computer cluster.

library(pbapply) # package that add progress bar to vectorized R functions ('*apply'). The implementation can easily be added to functions where showing the progress is useful (e.g. bootstrap). The type and style of the progress bar (with percentages or remaining time) can be set through options. Support several parallel process backends including future.

library(ranger) # a fast implementation of Random Forests, particularly suited for high dimensional data. Ensembles of classification, regression, survival and probability prediction trees are supported.

library(tidymodels) # collection of packages for modelling and machine learning using tydiverse principles.

library(e1071) # functions for latent class analysis, short time Fourier transform, fuzzy clustering, support vector machines, shortest path computation, bagged clustering, naive Bayes classifier, generalized k-nearest neighbour...

 

#Functions----

source("R/clean_name.R") #scorciatoia per :

# la funzione clean_name è progettata per pulire i nomi di un file e restituire un nome valido .che potrebbe essere utilizzato come nome di variabile o etichetta.
# In particolare è pensata per rimuovere caratteri non validi e garantire che il nome risultante sia utilizzabile come nome di variabile.
# ad esempio, se il nome fosse "123_examplefile", la funzione restituisce "x123examplefile"
# clean_name <- function(file) {                          
                                                         # tools::file_path_sans_ext() estrae il percorso del file senza l'estensione del file
#   var_name <- tools::file_path_sans_ext(basename(file)) 
                                                         # gsub() è una funzione utilizzata per sostituire pattern di testo in una stringa con altro testo o rimuoverli
                                                         # [^[:alnum:]_] è un espressione regolare che risponde a tutti i caratteri non alfanumerici eccetto "_". gsub() rimuove tali caratteri.
#   var_name <- gsub("[^[:alnum:]_]", "", var_name)
                                                         # grepl("^[0-9]", var_name) varifica se il nome inizia con un numero. In caso affermativo la funzione aggiunge un "x" davanti al nome
                                                         # per garantire che il nome sia valido in R
#   if (grepl("^[0-9]", var_name)) {

#     var_name <- paste0("x", var_name)

#   }

#   var_name

# }

 

#Load data----
# il seguente codice crea un elenco di file raster che corrispondono al modello "Ortho", assegna i loro nomi in base a una funzione 'clean_name', 
# e quindi li converte in oggetti raster utilizzando la funzione rast(). Il risultato è una lista di oggetti raster con nomi associati.
ortho <- list.files(pattern = "Ortho", full.names = TRUE, recursive = TRUE)%>% # this is a pipe operator, it takes the output of one function and passes it into another function as an argument.

  set_names(nm = map(., clean_name)) %>%  # set_names(purrr) assegna nomi ai componenti di una lista. nm = a character vector of names to assign to the object.

  map(~rast(.))                           # applichiamo la funzione rast() a ciascun elemento della lista 'ortho'

 
# il seguente codice esegue una serie di operazioni per ottenere e leggere i file shapefile che corrispondono al modello di nome "Buffer" dalla directory specificata
# I risultati vengono restituiti come una lista di oggetti sf (oggetti spaziali) che rappresentano i dati spaziali contenuti nei shapefiles.
buffers <- list.files(pattern = "Buffer.*shp",                       # list.files = it produces a character vector of the names of files or directories in the named directory

                      full.names = TRUE, recursive = TRUE)%>%

  set_names(nm = map(., clean_name)) %>%    # il . significa per ogni oggetto contenuto nella lista

  map(~st_read(.))                       # map(): apply a function to each element of a vector. st_read: read simple features from file or database.
                                         # st_read è una funzione anonima che legge i dati da file shapefile
# remove EL2, EY3, RA2 (2, 5, 10)

buffers[[10]] <- NULL

buffers[[5]] <- NULL

buffers[[2]] <- NULL

 
# con la funzione map (purrr) applichiamo la funzione ext() a ciascun elemento del vettore buffers. La funzione ext() restituirà l'estensione spaziale di ciascun oggetto della lista buffers.
# map2 (purrr) applica la funzione crop() ai dati dell'oggetto ortho e ai risultati ottenuti dalla prima parte.
# La funzione crop(.x, .y) prende due argomenti: il primo argomento (ortho) è il dato che verrà ritagliato, il secondo argomento ('...') è l'estensione specificata per il ritaglio.
# In pratica la funzione crop() sta ritagliando il dato ortho in base alle estensioni spaziali specificate nell'output della prima parte.
# In sintesi ritagliamo i dati spaziali nell'oggetto ortho con l'estensione dell'oggetto buffers.

ortho<-map2(ortho, map(buffers, ~ ext(.x)), ~ crop(.x, .y))    # ext(): get a SpatExtent of a SpatRaster, SpatVector or other spatial objects.


# la seguente linea di codice esegue un'operazione di mascheramento su dati spaziali rappresentati in ortho.
# Per ogni coppia di elementi da ortho e buffers la funzione mask() viene applicata per mascherare il dato spaziale in ortho utilizzando l'oggeto spaziale in buffers.
# Il risulatato viene assegnato nuovamente ad ortho sovrascrivendo il dato originale con i dati mascherati.
# Il mascheramento può essere utilizzato per escludere o isolare una determinata area di interesse nei dati spaziali.

ortho<-map2(ortho, buffers, ~ mask(.x, .y))                    # map2: map over multiple inputs simultaneously. Mask x = ortho utilizzando y = buffers

# mask(): create a new Raster object that has the same values as 'x', except for the cells that are 'NA' in a 'mask'. These cells become 'NA'. The mask can be either another Raster object of the same extent and resolution, or a Spatial* object in which 
case all cells are not covered by the spatial object are set to 'updatevalue'. You can use 'inverse=TRUE' to set the cells that are not 'NA' (or other 'maskvalue') in the mask, or not covered by the Spatial*object, to 'NA' (or other update value).

#Vogliamo sapere se lo sfondo del nostro transetto sia composto da NA per

#escluderli successivamente

 

outside_na <- map2(ortho, buffers, function(ortho, buffer) {    # = apply the function that is being created to the vectors ortho and buffers

  # Converti il buffer in SpatVector, is used as a spatial filter to select geometries to read.

  buffer <- terra::vect(buffer)

 

  # Crea un raster di stesse dimensioni dell'immagine originale e riempilo con 1

  base_raster <- ortho * 0 + 1

 

  # Applica la maschera all'immagine creata

  mask_inverse <- terra::mask(base_raster, buffer, inverse = TRUE)

 

  # Estrae i valori al di fuori del buffer

  outside_values <- terra::values(mask_inverse)

 

  # Verifica se questi valori sono tutti NA

  all(is.na(outside_values))

})

 

# Stampa i risultati

print(outside_na)

 

 

#Fiori ed Erba

 

 
# il seguente codice esegue una serie di operazioni per ottenere e leggere i file shpefile che corrispondono al modello di nome "fiori" dalla directory specificata.
# I risultati vengono restituiti come una lista di oggetti sf(oggetti spaziali) che rappresentano i dati spaziali contenuti nel file shapefile.
# Inoltre, vengono rimossi eventuali geometrie vuote o non valide. Questi dati spaziali possono includere geometrie (punti, linee, poligoni) e attributi associati.
fiori<- list.files(pattern = "fiori.*shp", full.names = TRUE, recursive = TRUE)%>%

  set_names(nm = map(., clean_name)) %>%  # set_names assegna nomi ai componenti di una lista

  map(~st_read(.))%>%                    

  map(~.[!st_is_empty(.), ])           # st_is_empty return for each geometry whether it is empty. ! is the logical operator for negation: !(5>1) return FALSE while "!(1>5)" returns TRUE
                                       # questa parte di codice sembra essere utilizzata per rimuovere le geometrie vuote o non valide da ciascun shapefile.
                                       # st_is_empty() verifica se una geometria è vuota. 
                                       # [!st_is_empty(.), ] è una notazione di selezione che estrae solo le righe delle geometrie che sono vuote. 

 

erba<-list.files(pattern = "erba.*shp", full.names = TRUE, recursive = TRUE)%>%

  set_names(nm = map(., clean_name)) %>%   # list.files produces a character vector of the names of files or directories in the named directory

  map(~st_read(.))%>%                      # read simple features from file or database, or retrieve layer names and their geometry type(s)

  map(~.[!st_is_empty(.), ]) 

 

#Ordiniamo i nomi in modo che coincidano

names(ortho) <- sort(names(ortho))        # order a vector or factor into ascending or descending order.

names(fiori) <- sort(names(fiori))

names(erba) <- sort(names(erba))

 

#Values extraction----
# estraiamo i valori di ortho relativi ai pixels interni ai poligoni classificati come fiori
df_fiori <- map2(ortho, fiori, ~terra::extract(.x, .y))     # extract a character column into multiple columns using regular regression groups
# stesso per pixels appartenenti ai poligoni di erba
df_erba <- map2(ortho, erba, ~terra::extract(.x, .y))

 

 

#Nomi delle colonne di ogni dataframe
# il seguente codice utilizza map() per iterare attraverso i dataframe della lista 'df_fiori' e assegna nuovi nomi alle colonne
# di ciascun dataframe. Questo può essere utile per rinominare le colonne in modo coerente e significativo in tutti i dataframe 
# presenti nella lista.
df_fiori <- map(df_fiori, ~{     # gli ultimi due caratteri indicano una funzione anonima che verrà applicata a ciascun dataframe della lista df_fiori

  colnames(.) <- c("ID_poly", paste0("banda_", 1:(ncol(.)-1)))      # sceglie nomi per ciascuna colonna del dataframe
# l'operazione assegna un vettore di nomi alle colonne. Il vettore include "ID_poly" come primo nome di colonna e poi nomi di colonne generati dinamicamente 
# per le restanti colonne. Questi nomi vengono creati utilizzando paste0() che combina la stringa "banda_" con numeri che vanno da 1 al numero totale di
# colonne del dataframe - 1, "ID_poly" non è incluso nel conteggio.
  .

})

 

df_erba <- map(df_erba, ~{

  colnames(.) <- c("ID_poly", paste0("banda_", 1:(ncol(.)-1)))

  .

})

 

 

#label: aggiungiamo una colonna contenente la label per ciascun dataframe
# il seguente codice aggiunge una colonna "label" chiamata "fiore" a ciascun dataframe all'interno della lista "df_fiori".
df_fiori <- map(df_fiori, ~{

  .$label <- "fiore"

  .

})

 

df_erba <- map(df_erba, ~{

  .$label <- "erba"

  .

})

 

#Bind dataframes----

 

# Unisci tutti i dataframe in una lista
# .id ="origin" intende aggiungere una nuova colonna dove per ogni pixel sia identificata l'immagine di appartenza
# il codice combina tutti i dataframe della lista 'df_fiori' in un unico dataframe. La nuova colonna "origin" verrà aggiunta al dataframe
# risultante per tenere traccia dell'origine di ciascuna riga.
df_fiori_unified <- bind_rows(df_fiori, .id = "origin")

df_erba_unified <- bind_rows(df_erba,.id = "origin")

 

 

# Numero di righe da campionare da ciascun gruppo

sample_size <- 90000

 

# Crea un campione bilanciato
# il codice esegue il campionamento casuale con sostituzione di un numero specifico di righe dal dataframe 'df_erba_unified'
# previamente raggruppato in base alla colonna "origin". Dopo il campionamento i dati vengono disaggregati (ungrouped), condentendo ulteriori analisi o operazioni sul dataframe
# in un formato non raggruppato.
df_erba_unified <- df_erba_unified %>%

  group_by(origin) %>%

  sample_n(size = sample_size, replace = TRUE) %>%    # sample_n() (dplyr) è usata per campionare un numero specifico di righe da un dataframe. 
# replace = TRUE signica che la funzione campionerà casualmente un numero di righe pari a sample size utilizzando la tecnica del campionamento con sostituzione
# il che significa che le stesse righe potrebbero apparire più di una volta nel campione risultante. È utile quando si desidera effettuare campionamenti casuali ripetuti 
# o quando si vuole consentire la possibilità di estrarre più volte le stesse righe.

  ungroup()    # consente di tornare al data frame originale senza gruppi, rendendo più facile eseguire ulteriori operazioni o analisi sui dati in modo non raggruppato

 

 
# il codice esegue la combinazione (concatenazione verticale) di due dataframe 'df_fiori_unified' e 'df_erba_unified' in un unico dataframe 'df_tot'.
# Successivamente trasforma l'ID della riga in una colonna separata chiamata 'ID_pixel' e rinomina la colonna dell'ID della riga. Il risultato è un unico 
# dataframe che contiene i dati combinati e l'ID delle righe rinominato come "ID_pixel". Questo tipo di manipolazione è spesso utile come preparazione dei 
# dati per l'analisi o l'apprendimento automatico.
df_tot <- rbind(df_fiori_unified, df_erba_unified) %>%

  rowid_to_column(.) %>%            # funzione del pacchetto dplyr che serve a convertire le righe ID in una colonna separata all'interno del dataframe.
                                      è utile quando si desidera trasformare l'ID della riga in una variabile del dataframe, rendendola parte dei dati.

  rename(ID_pixel = rowid)

 

# Crea un dataframe 'lungo' #SOLO SE VEDIAMO NA

# df_long <- df_tot %>%

#   tidyr::pivot_longer(cols =c("banda_1", "banda_2", "banda_3"),

#                       names_to = "banda",

#                       values_to = "valore")

# df_long %>%

#   filter(is.na(valore)) %>%

#   view()

 

 

#Splitting into train and test----

#Seleziono le bande

 

set.seed(123)

 

# Crea un nuovo dataframe con solo le colonne che vuoi usare

df_selected <- df_tot %>%

  select(banda_1,banda_2 , banda_3, label)     # funzione del pacchetto dplyr utilizzata per selezionare e restituire un sottoinsieme specifico delle colonne da un dataframe.
                                                 select(data, col1, col2, ...)

 

# Dividi il dataframe selezionato in set di addestramento e di test
# Il codice esegue la suddivisione iniziale di un dataframe 'df_selected' in un set di addestramento e un set di test. Il set di addestramento conterrà il 70% dei dati totali, 
# e il set di test conterrà il restante 30%. La suddivisione viene fatta in modo casuale, ma cercando di mantenere la stessa distribuzione delle etichette 'label' nei due set,
# se specificato. Questa divisione è comune nell'apprendimento automatico per allenare un modello sui dati di addestramento e valutarlo sui dati di test al fine di misurarne 
# le prestazioni. 
data_split <- initial_split(df_selected, prop = 0.7, strata = "label")   # prendiamo il 70% dei dati in df_selected per utilizzarli come set di training
# strata = "label" è un argomento opzionale che indica di suddividere i dati in modo da mantenere un bilanciamento rispetto alla colonna 'label'. Questo significa che la suddivisione
# casuale cercherà di mantenere la stessa distribuzione delle etichette "label" nel set di addestramento e test.

training_data <- training(data_split)    # estraiamo i dati di addestramento da quel 70% del totale

test_data <- testing(data_split)     # estraiamo i dati di test dallo stesso 70% del totale

 

# Convertiamo in un fattore
# la conversione delle colonne 'label' in fattori è spesso necessaria quando si lavora con dati categorici o di classificazione nell'apprendimento automatico.
# I modelli di apprendimento automatico spesso richiedono che le variabili di classe siano di tipo "fattore" per un corretto funzionamento. La conversione di
# un tipo di dati di tipo "fattore" è un passo comune nella preparazione dei dati per l'addestramento di modelli di classificazione.
training_data$label <- as.factor(training_data$label)

test_data$label <- as.factor(test_data$label)

 

#RF Model----

# la funzione expand.grid è utilizzata per creare un dataframe che rappresenta tutte le combinazioni possibili di un insieme di vettori o fattori forniti come argomenti.
# Questo è particolarmente utile quando si desidera esplorare tutte le combinazioni di variabili in un esperimento o in una griglia di parametri per la ricerca di modelli.
# le seguenti linee di codice creano un "tune grid" utilizzato per l'ottimizzazione dei parametri in un modello di Random Forest
# o un modello basato su alberi. Questo tunegrid rappresenta una griglia di combinazioni dei parametri da valutare al fine di determinare quale combinazione
# produce le migliori prestazioni per il modello. 

tuneGrid <- expand.grid(.mtry = c(1, 2, 3),    # .mtry rappresenta il numero di variabili casuali da selezionare per la suddivisione dei nodi durante la costruzione di alberi casuali. In questo caso consideriamo tre valori possibili: 1, 2 e 3.

                        .splitrule = "hellinger",     # spitrule è un parametro che specifica la regola da utilizzare per la suddivisione dei nodi degli alberi casuali. 

                        .min.node.size = c(1, 3, 5))     # min.node.size rappresenta la dimensione minima di un nodo nell'albero. Questo parametro controlla quante osservazioni minime devono essere presenti in un nodo prima che possa essere suddiviso ulteriormente. Qui si stanno considerando 3 possibili valori: 1, 3 e 5

 

# tuneGrid <- expand.grid(.mtry = c(1, 2, 3),

#                         .splitrule ="gini",

#                         .min.node.size = c(1, 3, 5))

 

# Define cross-validation strategy
# le seguenti linee di codice configurano i parametri di controllo per l'addestramento di modelli di machine learning
# utilizzando il pacchetto caret. In particolare, questi parametri vengono utilizzati durante il processo di cross-validation
# per la valutazione delle prestazioni del modello.
# La cross-validation verrà eseguita con 10 folds (ripetuto 10 volte), le previsioni verranno salvate alla fine di ogni fold e verranno visualizzati messaggi dettagliati durante il processo di 
# addestramento. Questi parametri sono utilizzati per gestire il processo di addestramento e valutazione del modello in modo più controllato e informato.

train_control <- trainControl(method = "cv", number = 10,   # method specifica il metodo di cross-validation da utilizzare. CV sta per cross-validation. 
                                                            # la cv è un approccio comune per valutare le prestazioni di un modello, suddividendo il dataset in diverse parti
                                                            # e addestrando il modello su una parte e valutandolo sull'altra, ripetendo questo processo più volte.

                              savePredictions = "final",    # savePredictions è un parametro che specifica quando le previsioni dovrebbero essere salvate. 
                                                            # Final indica che le previsioni saranno salvate alla fine del processo di cross-validation.

                              verboseIter = TRUE)    # parametro booleano che controlla se dovrebbero essere visualizzati messaggi di output dettagliati durante il processo di addestramento del modello.

 

## Train the model using ranger and the tuning grid

# Calcola i pesi delle classi
# il codice seguente estrae le classi uniche da un dataset di addestramento e crea un vettore di pesi per ciascuna classe, assegnando pesi specifici
# alle classi in modo da influenzare il comportamento del modello di classificazione durante l'addestramento.

classi <- unique(training_data$label)  # la funzione unique() viene usata per estrarre i valori unici dalla colonna 'label' del dataframe 'training data'.
                                       # I valori unici vengono quindi assegnati all'oggetto 'classi', creando un vettore con le etichette uniche delle classi.

class_weights <- c("erba" = 0.1, "fiore" = 0.9)  # qui si assegnano i pesi specifici a ciascuna classe del problema di classificazione. Attribuendo un peso maggiore ad una classe,
                                                 # il modello darà più importanza a quella classe nella fase di addestramento e ottimizzazione dei parametri.

 

# Addestra il modello utilizzando i pesi delle classi
# il codice addestra un modello Random Forest utilizzando il pacchetto caret e il metodo ranger.
# train() è una funzione del pacchetto caret utilizzata per addestrare modelli di machine learning.

RF <- train(label ~ ., data = training_data,     # label ~ indica che label è la variabile di risposta da prevedere e "." significa che tutte le altre variabili nel dataframe 'training data' sono utilizzate come variabili predittorie.

            method = "ranger",                   # method indica il metodo di addestramento da utilizzare, in questo caso il random forest.

            num.trees = 500,                     # num.trees specifica il numero di alberi da creare nel modello RF.

            trControl = train_control,           # trControl specifica i parametri di controllo dell'addestramento come i dettagli della cross-validation, l'output dettagliato ecc.

            tuneGrid = tuneGrid,                 # tuneGrid è l'oggetto precedentemente creato ed utilizzato per l'ottimizzazione dei parametri del modello.

            class.weights = class_weights)

 

# t0<-Sys.time()

# RF <- train(label ~ ., data = training_data,

#             method = "ranger",

#             num.trees = 500,

#             trControl = train_control,

#             tuneGrid = tuneGrid)

# t1<-Sys.time()

# t1-t0

 

print(RF)

 

#se volessi addestrare un modello SVM, potrei usare method = "svmLinear"

 

#RF model test----
# le seguenti linee di codice servono a valutare le prestazioni del modello di classificazione calcolando l'accuratezza,
# il richiamo e la misura F1 basati sulle previsioni del modello e le etichette reali dei dataset di test.
# Queste metriche forniscono una valutazione completa delle capacità del modello nel discrimare e classificare
# correttamente le istanze nelle classi target.

predizioni <- predict(RF, test_data)

matrice_confusione <- confusionMatrix(predizioni, test_data$label, positive = "fiore")

print(matrice_confusione)

accuracy <- matrice_confusione$overall['Accuracy']

recall <- matrice_confusione$byClass["Sensitivity"]

f1 <- matrice_confusione$byClass["F1"]


# Il seguente codice calcola alcune metriche di valutazione delle prestazioni di un modello di classificazione. 
auc_roc<- roc(test_data$label, as.numeric(predizioni))      # calcola la curva ROC (Receiver Operating Characteristics)
                                                            # la curva rappresenta la relazione tra la true positive rate e la false positive rate al variare del threshold di classificazione.

auc <- auc(auc_roc)              # questa calcola l'Area Under the Curve (AUC) della curva ROC. L'AUC è una misura dell'efficacia del modello
                                 # nel discriminare tra le classi positive e negative

precision <- matrice_confusione$byClass["Pos Pred Value"]    # estrae il valore di precisione del risultato della matrice di confusione.
                                                             # La precisione è la proporzione di istanze previste come positive che sono effettivamente positive.
# AUC e precision sono metriche spesso utilizzate per valutare le prestazioni di modelli di classificazione binaria.

 

 

print(matrice_confusione)

print(paste("Accuracy: ", accuracy))

print(paste("Recall: ", recall))

print(paste("F1-Score: ", f1))

print(paste("AUC-ROC: ", auc))

print(paste("Precision: ", precision))

 

 

 

#Funzione a tutti gli elementi in ortho
# il seguente codice rinomina le colonne per ciascun oggetto della lista 'ortho' in modo che le prime tre colonne abbiano lo stesso nome
# di quelle presenti in 'df_selected'.

ortho <- map(ortho, function(x) {    # x rappresenta ciascun oggetto della lista 'ortho'

  names(x) <- colnames(df_selected)[1:3]

  return(x)

})

 

 

#Application of model and save----

t0<-Sys.time()

# il seguente codice utilizza la funzione pbapply() per applicare il modello di classificazione RF a ciascun elemento
# della lista 'ortho', effettuando previsioni di classificazione per ciascun elemento memorizzando le previsioni in una lista.

classificazione_RF_list <- pbapply::pblapply(names(ortho), function(name) {    # pbapply() è una versione parallelizzata di 'lapply()'

  terra::predict(ortho[[name]], RF, na.rm=TRUE)       #gli NA sono dello sfondo, quindi possiamo escluderli. 'name' rappresenta ciascun elemento all'interno della lista 'ortho'

})

t1<-Sys.time()

t1-t0

 

#Time difference of 30.24887 mins

 

#QUI

 

#classificazione_RF_list <- future_map(ortho[[1]], ~predict(., RF),.progress = TRUE)

 

 

# Salva la classificazione come un file GeoTIFF

#output_filename <- "classificazione_RF.tif"

#writeRaster(classificazione_RF, output_filename, overwrite = TRUE)

 

 

names(classificazione_RF_list) <- names(ortho)  # Assicuriamoci che i nomi corrispondano

 

# il seguente codice itera attraverso i nomi degli elementi in 'classificazione_RF_list' e per ciascun elemento, 
# utilzza la funzione terra::writeRaster() per scrivere i dati raster associati su disco in file separati in formato TIFF.

walk(names(classificazione_RF_list), function(name) {      # la funzione 'walk' viene usata per iterare sui nomi della lista

# la differenza principale tra walk() e lapply() sta nell'output previsto. lapply() è usato per raccogliere i risultati
# delle operazioni, mentre walk() è utilizzato per effettuare operazioni senza la necessità di raccogliere risultati specifici.

terra::writeRaster(classificazione_RF_list[[name]],

                     filename = paste0(name, ".tif"),

                     overwrite=TRUE)

})

 

 

 

 

 

#ATTENZIONE proviamo a contare le aree----

 

# Carico la libreria

library(terra)

 

# Assumendo che i raster che devi analizzare siano già presenti nel tuo environment,

# non c'è bisogno di listare e caricare i file come hai fatto nel tuo script originale.

 

# Creo una struttura dati vuota per memorizzare i risultati

res_fiori <- data.frame(

  fiori = numeric(0),

  erba = numeric(0),

  area = character(0)

)

 

# Supponendo che i tuoi raster siano in un oggetto chiamato "classificazione_RF_list"
# il seguente codice itera attraverso tutti gli elementi nella lista 'classificazione_RF_list' e per ciascun elemento,
# estrae il raster e il nome dell'area associata. In seguito, calcola il numero di celle (pixel) nel raster, fornendo
# informazioni sulla dimensione o risoluzione del raster.
# Questo può essere utile quando si desidera ottenre informazioni sulla dimensione delle immmagini raster.

for (i in 1:length(classificazione_RF_list)) {

  raster_current <- classificazione_RF_list[[i]]   # estrae l'elemento raster corrispondente all'indice 'i', l'elemento viene memorizzato nella variabile raster_current

 areaname <- names(classificazione_RF_list)[i]     # è utilizzato per estrarre il nome dell'elemento corrente nella lista 'classificazione_RF:list' all'indice i
                                                   # il nome dell'elemento viene memorizzato nella variabile area_name
  
ncell(raster_current)         # ncell() è funzione che restituisce il numero di pixel presenti in un raster.

 

  # Convertire il raster in poligoni

  p <- as.polygons(raster_current)

 

  # Contare le celle per ogni categoria

  count <- expanse(p)             # calcola il numero di celle (pixel) per ciascuna categoria (fiori ed erba) all'interno dei poligoni.

 

  # Assumendo che "fiori" siano etichettati come 1 e "erba" come 2 nel tuo raster
  # estrae il conteggio di celle per le categorie fiori (indice 1) ed erba (indice 2) dai risultati del passo precedente.
  
  count_fiori <- count[1]            

  count_erba <- count[2]

 

  # Aggiungere i risultati al dataframe

  res_fiori <- rbind(res_fiori, data.frame(

    fiori = count_fiori,

    erba = count_erba,

    area = areaname

  ))

}

 

# Salvare i risultati

write.table(res_fiori, "res_fiori.csv", row.names = FALSE)
